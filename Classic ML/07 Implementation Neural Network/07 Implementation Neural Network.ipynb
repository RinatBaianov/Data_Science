{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "874c4783",
   "metadata": {},
   "source": [
    "<a href=\"#1-Definition\" style=\"margin-left: 0px;\">1 Definition</a>   \n",
    "<a href=\"#2-The-types-of-neurons\" style=\"margin-left: 0px;\">2 The types of neurons</a>  \n",
    "<a href=\"#3-Activation-functions\" style=\"margin-left: 0px;\">3 Activation functions</a>  \n",
    "<a href=\"#4-Types-of-neural-networks\" style=\"margin-left: 0px;\">4 Types of neural networks</a>  \n",
    "<a href=\"#5-Back-Propagation\" style=\"margin-left: 0px;\">5 Back Propagation</a>  \n",
    "<a href=\"#6-Implementation\" style=\"margin-left: 0px;\">6 Implementation</a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d33baec",
   "metadata": {},
   "source": [
    "### 1 Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104a9e1",
   "metadata": {},
   "source": [
    "Нейронная сеть - последовательность нейроном, соединенных между собой синнапсами  \n",
    "\n",
    "Нейрон - вычислительная единица, которая получает информацию, производит вычисления и передает информацию дальше   \n",
    "\n",
    "Синапс - связь между нейронами с одним параметром: \"вес\"  \n",
    "    \n",
    "Полносвязная нейронная сеть - сеть, в которой нейрон связан с каждым нейроном предыдущего слоя  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb81f5c",
   "metadata": {},
   "source": [
    "### 2 The types of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df134e",
   "metadata": {},
   "source": [
    "Входной нейрон: Входной нейрон получает внешние входные данные и передает их следующему слою нейронов в нейронной сети. Он представляет начальный этап обработки информации.\n",
    "\n",
    "Скрытый нейрон: Скрытый нейрон расположен между входным и выходным слоями нейронной сети. Он обрабатывает информацию от входных нейронов и передает результаты другим нейронам в сети. Скрытые нейроны играют важную роль во внутренних вычислениях сети.\n",
    "\n",
    "Нейрон смещения: Нейрон смещения – это дополнительный нейрон, который не прямо связан с входами, но предоставляет постоянный вход для последующих нейронов. Он помогает настраивать границу принятия решений и повышает гибкость нейронной сети.\n",
    "\n",
    "Контекстный нейрон: Контекстный нейрон – это тип нейрона, который сохраняет и предоставляет контекстную информацию другим нейронам в рекуррентной нейронной сети (RNN). Он позволяет сети сохранять память о предыдущих входах и влияет на текущие вычисления.\n",
    "\n",
    "Выходной нейрон: Выходной нейрон получает входы от предыдущего слоя (скрытых нейронов или входных нейронов) и выдает окончательный вывод или прогноз нейронной сети. Он представляет финальный этап обработки информации в сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a5ad6",
   "metadata": {},
   "source": [
    "### 3 Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a5f87",
   "metadata": {},
   "source": [
    "| Название               | Функция                                                                 | Производная                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------|-----------------------------------------------------------------------------|\n",
    "| **Identity**           | $f(x) = x$                                                             | $f'(x) = 1$                                                                |\n",
    "| **Step**               | $$f(x) = \\begin{cases} 0, & x < 0 \\\\ 1, & x \\geq 0 \\end{cases}$$       | $$f'(x) = \\begin{cases} 0, & x \\neq 0 \\\\ \\text{не определена}, & x = 0 \\end{cases}$$ |\n",
    "| **Logistic**           | $f(x) = \\frac{1}{1+e^{-x}}$                                            | $f'(x) = f(x)(1-f(x))$                                                     |\n",
    "| **Tanh**               | $f(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$                             | $f'(x) = 1 - f^2(x)$                                                       |\n",
    "| **Arctan**             | $f(x) = \\tan^{-1}(x)$                                                  | $f'(x) = \\frac{1}{x^2+1}$                                                  |\n",
    "| **ReLU**               | $$f(x) = \\begin{cases} 0, & x < 0 \\\\ x, & x \\geq 0 \\end{cases}$$       | $$f'(x) = \\begin{cases} 0, & x < 0 \\\\ 1, & x \\geq 0 \\end{cases}$$          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d83c9",
   "metadata": {},
   "source": [
    "Функции активации:  \n",
    "1) Тождественная  \n",
    "2) Единичная ступенька  \n",
    "3) Логистическая  \n",
    "4) Гиперболический тангенс  \n",
    "5) Арктангенс  \n",
    "6) Relu  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22349129",
   "metadata": {},
   "source": [
    "### 4 Types of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ec795",
   "metadata": {},
   "source": [
    "#### 1) Perceptron (P)  \n",
    "#### 2) A feedforward neural network (FF) (A fully connected neural network - полносвязная нейронная есть)  \n",
    "#### 3) A Radial Basis Function Network (RBFN)\n",
    "\n",
    "Сеть радиально базисных функций - сеть, которая использует радиальные базисные функции, как функции активации.  \n",
    "Радиально базисная функция - вещественная функция, значение которой зависит только от расстояния до начала координат $Ф(x,c) = Ф(||x-c||)$, c - центр координат.  \n",
    "Норму можно использовать любую, например, Евклидово расстояние: $d(p,q) = \\sqrt{(p_{1}-q_{1})^{2}+...+(p_{n}-q_{n})^{2}}$  \n",
    "    \n",
    "#### 4) Recurrent Neural Networks   (RNN)  \n",
    "Рекуретные нейронные сети  \n",
    "#### 5) Convolutional Neural Networks  (CNN)  \n",
    "Сверточные нейронные сети  \n",
    "#### 6) Deconvolutional Neural Networks (DNN)  \n",
    "Деконволюционные нейронные сети   \n",
    "#### 7) Autoencoder  (AE)\n",
    "Автоэнкодер      \n",
    "#### 8) Generative Adversarial Networks  (GAN)   \n",
    "Генеративно-состязательные нейронные сети    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f3541",
   "metadata": {},
   "source": [
    " ### 5 Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15552b",
   "metadata": {},
   "source": [
    "#### Общий алгоритм:  \n",
    "1) Выбираем начальные веса [$w^{1}_{11}$,...,$w^{3}_{12}$], например, в диапазоне [-0.5;0.5]   \n",
    "2) На входной слой нейронной сети подаем вектор X  \n",
    "3) Двигаемся слева направо и рассчитываем все параметры значений на каждом нейроне.  \n",
    "4) Получаем рассчетное значение y    \n",
    "5) Рассчитываем градиент функции потерь и считаем ее значение, учитывая фактический отклик  \n",
    "6) Двигаемся справа налево и для каждого нейрона рассчитываем локальный градиент и обновляем веса  \n",
    "7) Доходим до начала нейросети  \n",
    "8) Подаем следующий вектор X  \n",
    "9) Проходим все все вектора X из выборки  \n",
    "10) Прошли одну эпоху.  \n",
    "11) Действуем по этому алгоритму пока ошибка не станет приемлемой  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb6d4d",
   "metadata": {},
   "source": [
    "### 6 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c5bf1",
   "metadata": {},
   "source": [
    "**Реализован класс, который считает нейронную сеть с полносвязными слоями со следующими функциями активации(скрытый и выходной слой, выходной слой это просто последний слой):**   \n",
    "- sigmoid,   \n",
    "- relu,   \n",
    "- tanh,   \n",
    "- leaky_relu  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc16cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50d26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Фиксируем случайные сиды\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3dbf675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"../00 Data/titanic.csv\")\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f3e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# Замена пропусков в Age медианным значением\n",
    "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "\n",
    "# Замена пропусков в Embarked модой\n",
    "data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n",
    "\n",
    "# Семейный размер\n",
    "data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "\n",
    "# Одиночка (1, если FamilySize = 1)\n",
    "data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Encoder\n",
    "encoder = TargetEncoder(cols=['Sex', 'Embarked', 'FamilySize'])\n",
    "\n",
    "data[['Sex', 'Embarked', 'FamilySize']] = encoder.fit_transform(\n",
    "    data[['Sex', 'Embarked', 'FamilySize']], \n",
    "    data['Survived']  # Целевая переменная\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ecbe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler для всех признаков\n",
    "features = data.columns.drop('Survived')\n",
    "data[features] = StandardScaler().fit_transform(data[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52275af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.737695</td>\n",
       "      <td>-0.565736</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>-0.539973</td>\n",
       "      <td>1.290322</td>\n",
       "      <td>-1.231645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>1.355574</td>\n",
       "      <td>0.663861</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>2.044556</td>\n",
       "      <td>1.290322</td>\n",
       "      <td>-1.231645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>1.355574</td>\n",
       "      <td>-0.258337</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>-0.539973</td>\n",
       "      <td>-0.687981</td>\n",
       "      <td>0.811922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.566107</td>\n",
       "      <td>1.355574</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>0.432793</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>-0.539973</td>\n",
       "      <td>1.290322</td>\n",
       "      <td>-1.231645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.737695</td>\n",
       "      <td>0.433312</td>\n",
       "      <td>-0.474545</td>\n",
       "      <td>-0.473674</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>-0.539973</td>\n",
       "      <td>-0.687981</td>\n",
       "      <td>0.811922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived    Pclass       Sex       Age     SibSp     Parch      Fare  \\\n",
       "0         0  0.827377 -0.737695 -0.565736  0.432793 -0.473674 -0.502445   \n",
       "1         1 -1.566107  1.355574  0.663861  0.432793 -0.473674  0.786845   \n",
       "2         1  0.827377  1.355574 -0.258337 -0.474545 -0.473674 -0.488854   \n",
       "3         1 -1.566107  1.355574  0.433312  0.432793 -0.473674  0.420730   \n",
       "4         0  0.827377 -0.737695  0.433312 -0.474545 -0.473674 -0.486337   \n",
       "\n",
       "   Embarked  FamilySize   IsAlone  \n",
       "0 -0.539973    1.290322 -1.231645  \n",
       "1  2.044556    1.290322 -1.231645  \n",
       "2 -0.539973   -0.687981  0.811922  \n",
       "3 -0.539973    1.290322 -1.231645  \n",
       "4 -0.539973   -0.687981  0.811922  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa4f85",
   "metadata": {},
   "source": [
    "### Scikit-learn. MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8069a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592f8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_MLP:  0.8888888888888888\n",
      "MLP Classifier AUC: 0.946609465375643\n",
      "CPU times: total: 3.86 s\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Обучаем MLPClassifier на X_Train и Y_Train\n",
    "\n",
    "# data.iloc[:,1:].shape[1] - входной слой\n",
    "# Первый скрытый слой содержит 100 нейронов (ReLU)\n",
    "# Второй скрытый слой содержит 50 нейронов (ReLU)\n",
    "\n",
    "# Выходной слой: 1 нейрон с сигмоидой (так как MLPClassifier автоматически использует sigmoid для бинарной классификации)\n",
    "\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42, max_iter=1000)\n",
    "mlp_classifier.fit(data.iloc[:,1:].values, data.iloc[:,0].values)\n",
    "\n",
    "# 2. Делаем предсказания для X_Train\n",
    "y_pred_MLP = mlp_classifier.predict(data.iloc[:,1:].values)\n",
    "\n",
    "# 3. accuracy\n",
    "accuracy_MLP = accuracy_score(data.iloc[:,0].values, y_pred_MLP)\n",
    "print(\"accuracy_MLP: \", accuracy_MLP)\n",
    "\n",
    "AUC_ScL = roc_auc_score(data.iloc[:,0].values, mlp_classifier.predict_proba(data.iloc[:,1:].values)[:, 1])  \n",
    "print(f\"MLP Classifier AUC: {AUC_ScL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4d830",
   "metadata": {},
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d941080",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "1 Обновление моментов  \n",
    "\n",
    "1.1 Входные веса  \n",
    "\n",
    "$$\n",
    "M_{input} = \\beta_1 M_{input} + (1 - \\beta_1) X^T G_{hidden}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{input} = \\beta_2 V_{input} + (1 - \\beta_2) (X^T G_{hidden})^2\n",
    "$$\n",
    "\n",
    "1.2 Выходные веса  \n",
    "\n",
    "$$\n",
    "M_{output} = \\beta_1 M_{output} + (1 - \\beta_1) H_{output}^T G_{output}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{output} = \\beta_2 V_{output} + (1 - \\beta_2) (H_{output}^T G_{output})^2\n",
    "$$\n",
    "\n",
    "2 Коррекция смещения  \n",
    "\n",
    "$$\n",
    "M_{input}^{corr} = \\frac{M_{input}}{1 - \\beta_1^{epoch + 1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{input}^{corr} = \\frac{V_{input}}{1 - \\beta_2^{epoch + 1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "M_{output}^{corr} = \\frac{M_{output}}{1 - \\beta_1^{epoch + 1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_{output}^{corr} = \\frac{V_{output}}{1 - \\beta_2^{epoch + 1}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868828d5",
   "metadata": {},
   "source": [
    "Мы вычисляем градиент функции потерь $L$ относительно параметров модели, и это включает:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W}$ — градиент по весам $W$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ — градиент по смещениям $b$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z}$ — градиент по линейному слою $z$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a}$ — градиент по активации $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae79fca",
   "metadata": {},
   "source": [
    "### Как происходит обучение ?  \n",
    "\n",
    "#### fit  \n",
    "\n",
    "1) self.forward(X_batch, training=True)\n",
    "Проходит через все нейронки \n",
    "\n",
    "2) self.backward(X_batch, y_batch, lr)\n",
    "\n",
    "пробегаем по всем слоям справа налево от выходного до  входного  \n",
    "\n",
    "for i in reversed(range(len(self.layers))):    \n",
    "\n",
    "Например:  \n",
    "self.a[0] - это входной слой  \n",
    "self.a[1] - это скрытый слой  \n",
    "self.a[2] - это выходной слой  \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W} =  \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "\n",
    "$z = W x + b$ — линейная часть.\n",
    "\n",
    "$a = \\sigma(z)$ — активация.\n",
    "\n",
    "\n",
    "1) $$\\frac{\\partial L}{\\partial a}$$\n",
    "\n",
    "Потери $L(a, y)$ — например, Бинарная кросс энтропия. (мы минимизируем эту функцию)  \n",
    "\n",
    "Считаем ошибку между фактом и предсказанием error = y_pred - y  \n",
    "это градиент (производная) функции потерь по выходному слою для бинарной кросс-энтропии (binary cross-entropy)  \n",
    "\n",
    "2) $$\\frac{\\partial a}{\\partial z}$$  \n",
    "\n",
    "Считаем локальный градиент слоя  \n",
    "\n",
    "#####  Пример: активация `sigmoid` и её производная\n",
    "\n",
    "\n",
    "Допустим, функция активации — **sigmoid**:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Её производная:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "Теперь возьмём вход в слой:\n",
    "\n",
    "мы рассматриваем случай для батча размером 1 и одной нейронки  \n",
    "\n",
    "z = np.array([0.1, 0.5, -0.3])\n",
    "\n",
    "z вектор значений до активации для одного объекта (батч размером 1) на каком-то конкретном слое.  \n",
    "\n",
    "2.1) s = sigmoid(z)  = [0.52497919, 0.62245933, 0.42555748]  \n",
    "\n",
    "2.2) grad = s * (1 - s) = [0.24937604, 0.23500371, 0.24445831]  \n",
    "\n",
    "3) $$\\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z}$$  \n",
    "\n",
    "delta = error * grad_act  \n",
    "\n",
    "error — ошибка на выходе текущего слоя.  \n",
    "\n",
    "grad_act — производная функции активации.  \n",
    "\n",
    "delta — локальный градиент слоя, который будет использоваться для обновления весов.   \n",
    "\n",
    "4) $$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "### Обновление весов на всех слоях: Как рассчитываются градиенты по весам?\n",
    "\n",
    "После того как мы вычислили локальный градиент слоя (delta), мы можем посчитать, как влияет вход на ошибку — то есть градиент по весам.\n",
    "\n",
    "Для этого используем следующее выражение:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Поскольку:\n",
    "\n",
    "$$\n",
    "z = W x + b\n",
    "$$\n",
    "\n",
    "то:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial W} = x\n",
    "$$\n",
    "\n",
    "Таким образом, итоговое выражение для градиента по весам:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = x^T \\cdot \\delta\n",
    "$$\n",
    "\n",
    "### Пояснение:\n",
    "\n",
    "- **$x$** — входные данные в слой (или активации предыдущего слоя).\n",
    "- **$\\delta$** — локальный градиент слоя (ошибка, которая была вычислена ранее).\n",
    "\n",
    "Если вход представляет собой матрицу батча **$X$** с размерностью **[batch_size, input_dim]**, а **$\\delta$** имеет размерность **[batch_size, output_dim]**, то итоговый градиент по весам будет вычисляться следующим образом:\n",
    "\n",
    "```python\n",
    "dw = np.dot(self.a[i].T, delta) / X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cad2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        \n",
    "        # Оптимизатор Adam\n",
    "        self.optimizer = \"adam\"\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-7\n",
    "        self.m_weights = []\n",
    "        self.v_weights = []\n",
    "        self.m_biases = []\n",
    "        self.v_biases = []\n",
    "        self.t = 0  # Счетчик шагов для Adam\n",
    "\n",
    "    # отвечает за инициализацию и добавление нового слоя в нейронную сеть\n",
    "    def add_layer(self, n_neurons, activation=None, input_dim=None):\n",
    "        \n",
    "        # нет указано количество слоев\n",
    "        if len(self.layers) == 0 and input_dim is None:\n",
    "            raise ValueError(\"Для первого слоя укажите input_dim\")\n",
    "            \n",
    "        self.layers.append({'n_neurons': n_neurons, 'activation': activation})\n",
    "        \n",
    "        \n",
    "        # scale — это коэффициент масштабирования для случайно инициализированных весов. Он определяет, насколько большими будут начальные значения весов в зависимости от:\n",
    "\n",
    "        # Количества нейронов в текущем слое\n",
    "\n",
    "        # Количества нейронов в предыдущем слое (или размерности входа для первого слоя)\n",
    "\n",
    "        # Типа функции активации\n",
    "        \n",
    "        \n",
    "        # Определяем размерность входа\n",
    "        if len(self.weights) == 0:  # Для первого слоя: сколько признаков (фичей) у входных данных (input_dim)\n",
    "            n_input = input_dim\n",
    "        else:  # Последующие слои (сколько нейронов было в предыдущем слое)\n",
    "            n_input = self.layers[-2]['n_neurons']\n",
    "\n",
    "        # Выбираем коэффициент масштабирования\n",
    "        if activation in ['relu', 'leaky_relu']:\n",
    "            scale = np.sqrt(2.0 / n_input)  # Инициализация He для ReLU\n",
    "        else:\n",
    "            scale = np.sqrt(1.0 / n_input)   # Инициализация упрощенная Xavier для остальных функций активаций\n",
    "            \n",
    "        # Создаем матрицу весов размером (n_input, n_neurons) \n",
    "        # со случайными значениями из нормального распределения,\n",
    "        # масштабированными коэффициентом scale\n",
    "        w = np.random.randn(n_input, n_neurons) * scale\n",
    "        \n",
    "        self.weights.append(w)\n",
    "        self.biases.append(np.zeros((1, n_neurons)))\n",
    "        \n",
    "        # Инициализация для Adam\n",
    "        self.m_weights.append(np.zeros_like(w))\n",
    "        self.v_weights.append(np.zeros_like(w))\n",
    "        self.m_biases.append(np.zeros((1, n_neurons)))\n",
    "        self.v_biases.append(np.zeros((1, n_neurons)))\n",
    "\n",
    "    # функция активации\n",
    "    def activate(self, x, activation):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        if activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return np.where(x > 0, x, 0.01 * x)\n",
    "        elif activation is None:\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "    # расчёт частных производных\n",
    "    def activate_derivative(self, x, activation):\n",
    "        if activation == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-x))\n",
    "            return s * (1 - s)\n",
    "        elif activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif activation == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif activation == 'leaky_relu':\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        elif activation is None:\n",
    "            return np.ones_like(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "            \n",
    "    # Принимает входные данные X (матрица размером [batch_size, input_dim]).\n",
    "\n",
    "    # Последовательно применяет к ним все слои сети.\n",
    "\n",
    "    # Возвращает выход последнего слоя (предсказания сети).\n",
    "    def forward(self, X, training=False):\n",
    "        if training:\n",
    "            self.a = [X] # self.a - предикты, вначале они X\n",
    "            self.z = []\n",
    "        \n",
    "        a = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            a = self.activate(z, layer['activation'])\n",
    "            \n",
    "            if training: # сохраняет промежуточные результаты при обучении (если training=True)\n",
    "                self.z.append(z)\n",
    "                self.a.append(a)\n",
    "                \n",
    "        return a\n",
    "\n",
    "    # функция ошибка бинарная кросс-энтропия\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    \n",
    "    # реализует обратное распространение ошибки (backpropagation) - результат обновление весов\n",
    "    \n",
    "    # Вычисляет ошибку предсказания.\n",
    "    # Распространяет эту ошибку назад по слоям.\n",
    "    # Обновляет веса и смещения с помощью:\n",
    "    # Adam-оптимизатора (по умолчанию) Или обычного SGD (если оптимизатор не Adam).\n",
    "    \n",
    "    def backward(self, X, y, lr):\n",
    "        self.t += 1\n",
    "        y = y.reshape(-1, 1) # преобразует массив меток y в двумерный вектор-столбец [[1],[0],[1]]\n",
    "        y_pred = self.a[-1]\n",
    "        error = y_pred - y\n",
    "\n",
    "        #  пробегаем по всем слоям справа налево от входного до  выходного\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            \n",
    "            # расчёт частных производных для текущешго слоя\n",
    "            # Производная функции активации слоя в точке self.z[i] (выход слоя ДО применения активации)\n",
    "            grad_act = self.activate_derivative(self.z[i], self.layers[i]['activation']) \n",
    "            \n",
    "            # delta — это локальный градиент (или \"ошибка\") для текущего слоя\n",
    "            delta = error * grad_act\n",
    "\n",
    "            # градиенты весов\n",
    "            # self.a[i] — это выходные значения (активации) нейронов предыдущего слоя, которые являются входными значениями для текущего слоя.\n",
    "            dw = np.dot(self.a[i].T, delta) / X.shape[0]\n",
    "            \n",
    "            # градиенты смещений\n",
    "            db = np.mean(delta, axis=0, keepdims=True)\n",
    "\n",
    "            if self.optimizer == \"adam\":\n",
    "                # Обновление моментов для весов\n",
    "                self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * dw\n",
    "                self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (dw**2)\n",
    "                \n",
    "                # Обновление моментов для смещений\n",
    "                self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * db\n",
    "                self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (db**2)\n",
    "                \n",
    "                # Коррекция bias\n",
    "                m_hat_w = self.m_weights[i] / (1 - self.beta1**self.t)\n",
    "                v_hat_w = self.v_weights[i] / (1 - self.beta2**self.t)\n",
    "                \n",
    "                m_hat_b = self.m_biases[i] / (1 - self.beta1**self.t)\n",
    "                v_hat_b = self.v_biases[i] / (1 - self.beta2**self.t)\n",
    "                \n",
    "                # Обновление параметров\n",
    "                self.weights[i] -= lr * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "                self.biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "            else:\n",
    "                # Простой SGD\n",
    "                self.weights[i] -= lr * dw\n",
    "                self.biases[i] -= lr * db\n",
    "\n",
    "            if i > 0:\n",
    "                error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "    # обучает нейроесть\n",
    "    # нет автоматической остановки при достижении малой ошибки — обучение продолжается все указанные epochs\n",
    "    def fit(self, X, y, epochs=100, lr=0.001, batch_size=32, validation_data=None, verbose=1):\n",
    "        \n",
    "        #  переделываем паднас файлы в нампай\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.values\n",
    "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
    "            y = y.values\n",
    "         \n",
    "        # переделываем размерности в двумерную\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        # пробегеам по эпохам\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # перемешиваем строчки X и y\n",
    "            indices = np.random.permutation(len(X))\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            \n",
    "            # пробегаем по батчам\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                \n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # пробегаем слева направо и обратно справа налево обновляя веса\n",
    "                self.forward(X_batch, training=True)\n",
    "                self.backward(X_batch, y_batch, lr)\n",
    "                \n",
    "                # считаем ошибку и точность\n",
    "                y_pred = self.a[-1]\n",
    "                epoch_loss += self.compute_loss(y_batch, y_pred) * len(X_batch)\n",
    "                epoch_acc += accuracy_score(y_batch, (y_pred > 0.5).astype(int)) * len(X_batch)\n",
    "            \n",
    "            epoch_loss /= len(X)\n",
    "            epoch_acc /= len(X)\n",
    "            self.history['loss'].append(epoch_loss)\n",
    "            self.history['accuracy'].append(epoch_acc)\n",
    "            \n",
    "            # если есть валидационная выбора< то считаем все параметры и для нее\n",
    "            val_auc = None\n",
    "            if validation_data is not None:\n",
    "                X_val, y_val = validation_data\n",
    "                y_val_pred = self.predict_proba(X_val)\n",
    "                val_loss = self.compute_loss(y_val, y_val_pred.reshape(-1, 1))\n",
    "                val_acc = accuracy_score(y_val, (y_val_pred > 0.5).astype(int))\n",
    "                val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "                self.history['val_loss'].append(val_loss)\n",
    "                self.history['val_accuracy'].append(val_acc)\n",
    "            \n",
    "            # печатать или нет результаты\n",
    "            if verbose and epoch % verbose == 0:\n",
    "                msg = f\"Epoch {epoch}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.4f}\"\n",
    "                if validation_data:\n",
    "                    msg += f\" - val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f} - val_auc: {val_auc:.4f}\"\n",
    "                print(msg)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            X = X.values\n",
    "        return self.forward(X).flatten()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_proba = self.predict_proba(X)\n",
    "        acc = accuracy_score(y, y_pred)\n",
    "        auc = roc_auc_score(y, y_proba)\n",
    "        loss = self.compute_loss(y.reshape(-1, 1), y_proba.reshape(-1, 1))\n",
    "        return {'loss': loss, 'accuracy': acc, 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07cbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 - loss: 0.6447 - acc: 0.6195\n",
      "Epoch 1/100 - loss: 0.5322 - acc: 0.7351\n",
      "Epoch 2/100 - loss: 0.4964 - acc: 0.7688\n",
      "Epoch 3/100 - loss: 0.4718 - acc: 0.8058\n",
      "Epoch 4/100 - loss: 0.4573 - acc: 0.8081\n",
      "Epoch 5/100 - loss: 0.4519 - acc: 0.8058\n",
      "Epoch 6/100 - loss: 0.4407 - acc: 0.8126\n",
      "Epoch 7/100 - loss: 0.4348 - acc: 0.8182\n",
      "Epoch 8/100 - loss: 0.4284 - acc: 0.8171\n",
      "Epoch 9/100 - loss: 0.4224 - acc: 0.8182\n",
      "Epoch 10/100 - loss: 0.4252 - acc: 0.8227\n",
      "Epoch 11/100 - loss: 0.4158 - acc: 0.8215\n",
      "Epoch 12/100 - loss: 0.4121 - acc: 0.8260\n",
      "Epoch 13/100 - loss: 0.4140 - acc: 0.8204\n",
      "Epoch 14/100 - loss: 0.4070 - acc: 0.8339\n",
      "Epoch 15/100 - loss: 0.4018 - acc: 0.8328\n",
      "Epoch 16/100 - loss: 0.4047 - acc: 0.8316\n",
      "Epoch 17/100 - loss: 0.3989 - acc: 0.8305\n",
      "Epoch 18/100 - loss: 0.3971 - acc: 0.8350\n",
      "Epoch 19/100 - loss: 0.3925 - acc: 0.8373\n",
      "Epoch 20/100 - loss: 0.3901 - acc: 0.8328\n",
      "Epoch 21/100 - loss: 0.3904 - acc: 0.8328\n",
      "Epoch 22/100 - loss: 0.3881 - acc: 0.8395\n",
      "Epoch 23/100 - loss: 0.3893 - acc: 0.8418\n",
      "Epoch 24/100 - loss: 0.3886 - acc: 0.8440\n",
      "Epoch 25/100 - loss: 0.3832 - acc: 0.8384\n",
      "Epoch 26/100 - loss: 0.3811 - acc: 0.8429\n",
      "Epoch 27/100 - loss: 0.3830 - acc: 0.8406\n",
      "Epoch 28/100 - loss: 0.3835 - acc: 0.8395\n",
      "Epoch 29/100 - loss: 0.3772 - acc: 0.8429\n",
      "Epoch 30/100 - loss: 0.3739 - acc: 0.8485\n",
      "Epoch 31/100 - loss: 0.3687 - acc: 0.8530\n",
      "Epoch 32/100 - loss: 0.3729 - acc: 0.8462\n",
      "Epoch 33/100 - loss: 0.3725 - acc: 0.8451\n",
      "Epoch 34/100 - loss: 0.3686 - acc: 0.8507\n",
      "Epoch 35/100 - loss: 0.3767 - acc: 0.8496\n",
      "Epoch 36/100 - loss: 0.3671 - acc: 0.8541\n",
      "Epoch 37/100 - loss: 0.3664 - acc: 0.8552\n",
      "Epoch 38/100 - loss: 0.3665 - acc: 0.8530\n",
      "Epoch 39/100 - loss: 0.3623 - acc: 0.8552\n",
      "Epoch 40/100 - loss: 0.3626 - acc: 0.8496\n",
      "Epoch 41/100 - loss: 0.3648 - acc: 0.8575\n",
      "Epoch 42/100 - loss: 0.3575 - acc: 0.8563\n",
      "Epoch 43/100 - loss: 0.3641 - acc: 0.8597\n",
      "Epoch 44/100 - loss: 0.3646 - acc: 0.8575\n",
      "Epoch 45/100 - loss: 0.3584 - acc: 0.8586\n",
      "Epoch 46/100 - loss: 0.3595 - acc: 0.8586\n",
      "Epoch 47/100 - loss: 0.3564 - acc: 0.8575\n",
      "Epoch 48/100 - loss: 0.3542 - acc: 0.8586\n",
      "Epoch 49/100 - loss: 0.3548 - acc: 0.8620\n",
      "Epoch 50/100 - loss: 0.3562 - acc: 0.8608\n",
      "Epoch 51/100 - loss: 0.3588 - acc: 0.8485\n",
      "Epoch 52/100 - loss: 0.3492 - acc: 0.8676\n",
      "Epoch 53/100 - loss: 0.3475 - acc: 0.8642\n",
      "Epoch 54/100 - loss: 0.3502 - acc: 0.8608\n",
      "Epoch 55/100 - loss: 0.3477 - acc: 0.8653\n",
      "Epoch 56/100 - loss: 0.3493 - acc: 0.8575\n",
      "Epoch 57/100 - loss: 0.3476 - acc: 0.8586\n",
      "Epoch 58/100 - loss: 0.3476 - acc: 0.8620\n",
      "Epoch 59/100 - loss: 0.3440 - acc: 0.8653\n",
      "Epoch 60/100 - loss: 0.3424 - acc: 0.8676\n",
      "Epoch 61/100 - loss: 0.3393 - acc: 0.8721\n",
      "Epoch 62/100 - loss: 0.3443 - acc: 0.8653\n",
      "Epoch 63/100 - loss: 0.3445 - acc: 0.8676\n",
      "Epoch 64/100 - loss: 0.3391 - acc: 0.8732\n",
      "Epoch 65/100 - loss: 0.3362 - acc: 0.8721\n",
      "Epoch 66/100 - loss: 0.3370 - acc: 0.8676\n",
      "Epoch 67/100 - loss: 0.3394 - acc: 0.8642\n",
      "Epoch 68/100 - loss: 0.3452 - acc: 0.8664\n",
      "Epoch 69/100 - loss: 0.3373 - acc: 0.8676\n",
      "Epoch 70/100 - loss: 0.3398 - acc: 0.8732\n",
      "Epoch 71/100 - loss: 0.3382 - acc: 0.8642\n",
      "Epoch 72/100 - loss: 0.3336 - acc: 0.8721\n",
      "Epoch 73/100 - loss: 0.3367 - acc: 0.8754\n",
      "Epoch 74/100 - loss: 0.3331 - acc: 0.8743\n",
      "Epoch 75/100 - loss: 0.3326 - acc: 0.8687\n",
      "Epoch 76/100 - loss: 0.3348 - acc: 0.8721\n",
      "Epoch 77/100 - loss: 0.3388 - acc: 0.8721\n",
      "Epoch 78/100 - loss: 0.3297 - acc: 0.8732\n",
      "Epoch 79/100 - loss: 0.3356 - acc: 0.8754\n",
      "Epoch 80/100 - loss: 0.3298 - acc: 0.8754\n",
      "Epoch 81/100 - loss: 0.3276 - acc: 0.8709\n",
      "Epoch 82/100 - loss: 0.3324 - acc: 0.8709\n",
      "Epoch 83/100 - loss: 0.3282 - acc: 0.8721\n",
      "Epoch 84/100 - loss: 0.3302 - acc: 0.8721\n",
      "Epoch 85/100 - loss: 0.3283 - acc: 0.8709\n",
      "Epoch 86/100 - loss: 0.3285 - acc: 0.8754\n",
      "Epoch 87/100 - loss: 0.3260 - acc: 0.8754\n",
      "Epoch 88/100 - loss: 0.3239 - acc: 0.8777\n",
      "Epoch 89/100 - loss: 0.3301 - acc: 0.8676\n",
      "Epoch 90/100 - loss: 0.3250 - acc: 0.8743\n",
      "Epoch 91/100 - loss: 0.3244 - acc: 0.8721\n",
      "Epoch 92/100 - loss: 0.3244 - acc: 0.8754\n",
      "Epoch 93/100 - loss: 0.3248 - acc: 0.8676\n",
      "Epoch 94/100 - loss: 0.3235 - acc: 0.8653\n",
      "Epoch 95/100 - loss: 0.3206 - acc: 0.8788\n",
      "Epoch 96/100 - loss: 0.3243 - acc: 0.8732\n",
      "Epoch 97/100 - loss: 0.3203 - acc: 0.8709\n",
      "Epoch 98/100 - loss: 0.3208 - acc: 0.8777\n",
      "Epoch 99/100 - loss: 0.3228 - acc: 0.8709\n",
      "Первые 10 предсказаний: [0 1 1 1 0 0 0 0 1 1]\n",
      "Accuracy на тренировочных данных: 0.8843995510662177\n",
      "CPU times: total: 3.3 s\n",
      "Wall time: 3.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Создаем и настраиваем сеть\n",
    "model = FlexNN()\n",
    "model.add_layer(input_dim=data.iloc[:,1:].values.shape[1], n_neurons=data.iloc[:,1:].shape[1], activation='relu')  # Входной слой (аналог первого Dense)\n",
    "model.add_layer(n_neurons=100, activation='relu')               # Скрытый слой (аналог второго Dense)\n",
    "model.add_layer(n_neurons=50, activation='relu')               # Скрытый слой (аналог второго Dense)\n",
    "model.add_layer(n_neurons=1, activation='sigmoid')             # Выходной слой (аналог третьего Dense)\n",
    "\n",
    "# 2. Обучение (без валидации)\n",
    "model.fit(data.iloc[:,1:].values, data.iloc[:,0].values, epochs=100)  # <-- Здесь нет validation_data\n",
    "\n",
    "# 3. Предсказание на тренировочных данных\n",
    "y_pred = model.predict(data.iloc[:,1:].values)\n",
    "print(\"Первые 10 предсказаний:\", y_pred[:10])\n",
    "print(\"Accuracy на тренировочных данных:\", accuracy_score(data.iloc[:,0].values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e40a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(data.iloc[:,1:].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "106bb4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9244\n"
     ]
    }
   ],
   "source": [
    "# 5. Оценка\n",
    "AUC_my = roc_auc_score(data.iloc[:,0].values, y_proba)\n",
    "print(f\"AUC: {AUC_my:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcb0f6",
   "metadata": {},
   "source": [
    "#### Результаты несколько отличаются, но в целом класс хорошо работает "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
