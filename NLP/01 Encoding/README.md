Кратко расписана теория для различных методов энкодинга

Был построен простой "игручешный" пример состоящий из трех различных предложений(документов), на котором было показано как работают различные методы энкодинга

Методы энкодинга:  

**1 Basic Vectorization Methods**    
1.1 One-hot Encoding  
1.2 Count Encoding (Bag of Words, BoW)  
1.3 TF-IDF (Term Frequency - Inverse Document Frequency)  

**2 Dense Embeddings**    
2.1 Word2Vec (CBOW, Skip-Gram)  
2.2 FastText (только теория)  
2.3 GloVe (Global Vectors for Word Representation) (только теория)    

**3 Contextual-Embeddings (Neural Networks)**    
3.1 BERT (Bidirectional Encoder Representations from Transformers)  
3.2 RoBERTa / DistilBERT / GPT / T5 / XLNet (только теория)     
